{"cells":[{"cell_type":"markdown","metadata":{"id":"-IPopZoocxrx"},"source":["# **bigscience/bloomz-7b1**"]},{"cell_type":"markdown","metadata":{"id":"A6DMT71Dc5qE"},"source":["# **Step 1: Libraries**"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5001,"status":"ok","timestamp":1710784251282,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"m1gb0V18crrH"},"outputs":[],"source":["!pip install langchain accelerate bitsandbytes --quiet"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710784255422,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"WsTPOa-vcuXl"},"outputs":[],"source":["import os\n","from tqdm import tqdm\n","import pandas as pd\n","import torch\n","import accelerate\n","from langchain import PromptTemplate, HuggingFacePipeline\n","from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline"]},{"cell_type":"markdown","metadata":{"id":"iY6s2y5Vd0WA"},"source":["# **Step 2: Load Data**"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84341,"status":"ok","timestamp":1710784219586,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"0_w7xQnJdzt-","outputId":"68ae0494-2ae0-4b1e-8972-070374e61220"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":466,"status":"ok","timestamp":1710784220050,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"aVK1vVBE2zpA"},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Medical_Transcription_Extraction/mtsamples.csv')"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1710784220050,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"zMWDGeoy28rN"},"outputs":[],"source":["transcription_list = df['transcription'].to_list()"]},{"cell_type":"markdown","metadata":{"id":"klji-HFNd58V"},"source":["# **Step 3: Model**"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710784220758,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"dwXEdul54lSc"},"outputs":[],"source":["quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"0PqfE5dNgchw"},"source":["**To get the huggingface token, click [here](https://huggingface.co/settings/tokens)**"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":560},"executionInfo":{"elapsed":1221,"status":"error","timestamp":1710784259576,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"3W1T51MkiUmi","outputId":"99fa3e00-3d9d-4817-df20-59c3ec9c0d29"},"outputs":[{"ename":"ImportError","evalue":"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-e0d64df20bab>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigscience/bloomz-7b1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"bigscience/bloomz-7b1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2943\u001b[0m                 )\n\u001b[1;32m   2944\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2945\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   2946\u001b[0m                     \u001b[0;34m\"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2947\u001b[0m                 )\n","\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-7b1\", trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"bigscience/bloomz-7b1\",\n","    quantization_config=quantization_config,\n","    low_cpu_mem_usage=True,\n","    device_map=\"auto\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1710784221397,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"U__pQWLkv9bx"},"outputs":[],"source":["pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    return_full_text=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1710784221398,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"cbp6ff6-2g6L"},"outputs":[],"source":["llm = HuggingFacePipeline(pipeline=pipe)"]},{"cell_type":"markdown","metadata":{"id":"hFHg5_GfeCu1"},"source":["# **Step 4: Inference**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1710784221398,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"gv9_X4Qe8AZA"},"outputs":[],"source":["def get_chatglm_response(transcription):\n","    # prompt = f\"\"\"This is a patient's transcription: '{transcription}'\n","    # For the given patient's transcription, do the next actions sequentially:\n","    # First, propose the patient's age. If there is no words about age, return unknown.\n","    # Second, propose no more than ten words that summarize the treatment. If you don't know the treatment, assume '''unknown'''\n","    # Note: don't use any other words.\n","    # Then, show your response following the next format:\n","    # Age: <age>\n","    # Treatment: The patient will get <treatment> as treatment.\n","    # \"\"\"\n","    prompt = f\"This is a patient's transcription: '{transcription}' \\n\\n Please use this format as the answer: The patient age is [Your extracted answer]. The patient will get [Your extracted answer] as treatment. Note: If the treatment has already been administered, it's also considered as the treatment, the format will be revised slightly: The patient got [Your extracted answer] as treatment. If there is no specific neither age or treatment , you can return unknown, unknown. If there is no specific age but treatment, you can return unknown, The patient will get [Your extracted answer] as treatment. There may be not only one treatment\"\n","    prompt = PromptTemplate.from_template(prompt)\n","    response = llm.invoke(prompt.format(), max_length=500)\n","    return response"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1710784221398,"user":{"displayName":"Hehua Fan","userId":"10532428628985434302"},"user_tz":240},"id":"Xn8aQTFo8WaL"},"outputs":[],"source":["res = []\n","for transcription in tqdm(transcription_list[0:2]):\n","    response = get_chatglm_response(transcription)\n","    res.append(response)\n","print(res)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNHK6x5qlYbMwo9zveO3IQi","collapsed_sections":["iY6s2y5Vd0WA"],"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1eVCtr5OK_z24FPFvvSiHj3JmftD6nsD7","timestamp":1710782181975},{"file_id":"1sM-S8XlQCjJ-OAiNCqhiWsBfksrzdYq6","timestamp":1710780245337}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
